{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7473c676-c866-4d23-8f14-d90592aaf478",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "Separating this out just so I can run multiple things at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1643a8d5-7c8d-4a4e-bcba-f838c4b176ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd50036-a9b4-432c-bb4d-f6b7299b3014",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "This section will introduce some scoring metrics!\n",
    "We don't really have information on the domain/range just yet, so it's going to be very simple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5aee4e-da93-4d40-a14b-6fa0e06812e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input scores look like this, where P17 is the variable relation label.\n",
    "# scores = {'relu': {'max': {'P17': {'hsd': [...]}}}}\n",
    "# The output scores need to instead look like this:\n",
    "# scores = {'relu': {'max': {'hsd': {'top_5': 0.0}}}}\n",
    "# Easy enough.\n",
    "\n",
    "\n",
    "def score_fixer(subscores, doc, rel, flipsort=False, dr=False, task_name='docred'):\n",
    "    \"\"\"\n",
    "    No, we're not cheating here. By \"fixing\" the scores, we're actually trying to make things more fair.\n",
    "    We want to try and present only one candidate per entity pair, but we generate candidates at the\n",
    "    mention level, which can greatly inflate the number of true (and false) statements.\n",
    "    So, this function filters the candidates by the highest-scoring version.\n",
    "    It also filters candidates by domain and range restrictions, in case that didn't happen earlier.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    res_scores = []\n",
    "    list_sort = list(sorted(subscores, key=lambda x:x[-1]) if flipsort else sorted(subscores, key=lambda x:-x[-1]))\n",
    "    # print(list_sort)\n",
    "    count_true = 0\n",
    "    count_false = 0\n",
    "    first_true = -1\n",
    "    i = 0\n",
    "    for score in list_sort:\n",
    "        ex = score[0:2]\n",
    "        # If we've seen this pair before, then it had a higher score, so skip it.\n",
    "        # We include the tag because there are some documents which have the same mentions marked for multiple\n",
    "        # entities, so the pairs could simultaneously be correct and incorrect... it's weird.\n",
    "        if ex not in seen:\n",
    "            if dr:\n",
    "                # Check the domain and range calculated elsewhere to make sure this should be kept.\n",
    "                if ex in dr[doc][rel]:\n",
    "                    if score[2]:\n",
    "                        count_true += 1\n",
    "                        if first_true == -1:\n",
    "                            first_true = i\n",
    "                    else:\n",
    "                        count_false += 1\n",
    "                    seen.add(ex)\n",
    "                    res_scores.append(score)\n",
    "                    i += 1\n",
    "            else:\n",
    "                if score[2]:\n",
    "                    count_true += 1\n",
    "                    first_true = min(first_true, i)\n",
    "                else:\n",
    "                    count_false += 1\n",
    "                seen.add(ex)\n",
    "                res_scores.append(score)\n",
    "                i += 1\n",
    "    return res_scores, first_true, count_true, count_false, i\n",
    "\n",
    "\n",
    "def counts_to_percent(fullscores):\n",
    "    rel_len = {}\n",
    "    for rel in fullscores:\n",
    "        if rel != 'total_docs':\n",
    "            for nlpl in fullscores[rel]:\n",
    "                for tk in fullscores[rel][nlpl]:\n",
    "                    for scr in fullscores[rel][nlpl][tk]:\n",
    "                        if rel not in rel_len:\n",
    "                            rel_len[rel] = fullscores[rel][nlpl][tk][scr]['total_docs']\n",
    "                        if fullscores[rel][nlpl][tk][scr]['total_docs'] > 0:\n",
    "                            if 'ratioB' == tk:\n",
    "                                fullscores[rel][nlpl][tk][scr] = (fullscores[rel][nlpl]['#true'][scr]['correct'] / fullscores[rel][nlpl]['#false'][scr]['correct'])*100.0\n",
    "                            elif '#' not in tk:\n",
    "                                fullscores[rel][nlpl][tk][scr] = (fullscores[rel][nlpl][tk][scr]['correct'] / fullscores[rel][nlpl][tk][scr]['total_docs'])*100.0\n",
    "                            else:\n",
    "                                fullscores[rel][nlpl][tk][scr] = (fullscores[rel][nlpl][tk][scr]['correct'] / fullscores[rel][nlpl][tk][scr]['total_docs'])\n",
    "                        else:\n",
    "                            fullscores[rel][nlpl][tk][scr] = -1\n",
    "    return fullscores, rel_len\n",
    "\n",
    "\n",
    "def top_k(folder, task_name, data_set, max_k=5, num_blanks=2, dr=None, max_doc=1000, masks=False, n_passes=1, easymode=True, model_name=\"bert-large-cased\"):\n",
    "    out_scores = {'total_docs':0}\n",
    "    for d in range(max_doc):\n",
    "        # pth = f\"/home/riley/understanding-pll/res/20240614/{(task_name + '_') if task_name != 'docred' else ''}{(model_name + '_')\n",
    "        # if model_name != 'bert-large-cased' else ''}{data_set}_{d}{'_' + str(num_blanks) + 'blanks' if num_blanks != 2 else ''}_MASK{'' if n_passes == 1 else '_'+str(n_passes)}.pickle\"\n",
    "        pth = f\"/home/riley/understanding-pll/res/{folder}/{task_name}_{model_name}_{data_set}_{d}_{num_blanks}b_{num_passes}p.pickle\"\n",
    "        if os.path.exists(pth):\n",
    "            if task_name == \"biored\" and model_name != \"bert-large-cased\":\n",
    "                pth2 = f\"/home/riley/understanding-pll/res/{folder}/{task_name}_bert-large-cased_{data_set}_{d}_{num_blanks}b_{num_passes}p.pickle\"\n",
    "                if not os.path.exists(pth2):\n",
    "                    continue\n",
    "            with open(pth, \"rb\") as pfile:\n",
    "                out_scores['total_docs'] += 1\n",
    "                in_scores = pickle.load(pfile)\n",
    "                for nl in in_scores:\n",
    "                    # if nl not in out_scores:\n",
    "                    #     out_scores[nl] = {}\n",
    "                    for pl in in_scores[nl]:\n",
    "                        nlpl = f\"{nl}+{pl}\"\n",
    "                        for rel in in_scores[nl][pl]:\n",
    "                            if rel not in out_scores:\n",
    "                                out_scores[rel] = {}    \n",
    "                            if nlpl not in out_scores[rel]:\n",
    "                                out_scores[rel][nlpl] = {}\n",
    "                            for scr in in_scores[nl][pl][rel]:\n",
    "                                sf, best, nt, nf, tot = score_fixer(in_scores[nl][pl][rel][scr], flipsort=(scr==\"msd\"), dr=dr, doc=d, rel=rel, task_name=task_name)\n",
    "                                # There are some documents where all correct answers get filtered out due to the domain and range constraints.\n",
    "                                # In this sense, the document does not represent the relation as defined by the schema/ontology, so we choose\n",
    "                                # to filter it out anyway.\n",
    "                                # The other conditions try to avoid \"easy mode\" documents which only inflate the scores.\n",
    "                                if best == -1 or tot < 10 or nf < 5:\n",
    "                                    continue\n",
    "                                tks = [f\"top_{k}\" for k in range(1, max_k+1)]\n",
    "\n",
    "                                for tk in [\"ratioB\", \"mrr\"] + tks + [\"ratioA\", \"#true\", \"#false\"]:\n",
    "                                    if tk not in out_scores[rel][nlpl]:\n",
    "                                        out_scores[rel][nlpl][tk] = {}\n",
    "                                    if scr not in out_scores[rel][nlpl][tk]:\n",
    "                                        out_scores[rel][nlpl][tk][scr] = {}\n",
    "                                        out_scores[rel][nlpl][tk][scr]['total_docs'] = 1\n",
    "                                        out_scores[rel][nlpl][tk][scr]['correct'] = 0\n",
    "                                    else:\n",
    "                                        out_scores[rel][nlpl][tk][scr]['total_docs'] += 1\n",
    "                                # best = trues.index(True)\n",
    "                                out_scores[rel][nlpl][\"mrr\"][scr]['correct'] += 1/(best + 1) if best != -1 else 0\n",
    "                                # Because, of course, there are some documents with *only* correct answers.\n",
    "                                if nf > 0:\n",
    "                                    out_scores[rel][nlpl][\"ratioA\"][scr]['correct'] += nt/nf\n",
    "                                else:\n",
    "                                    out_scores[rel][nlpl][\"ratioA\"][scr]['total_docs'] -= 1\n",
    "                                out_scores[rel][nlpl][\"#true\"][scr]['correct'] += nt\n",
    "                                out_scores[rel][nlpl][\"#false\"][scr]['correct'] += nf\n",
    "                                if best > -1 and best < max_k:\n",
    "                                    for k in range(best, max_k):\n",
    "                                        out_scores[rel][nlpl][f\"top_{k+1}\"][scr]['correct'] += 1\n",
    "                                    \n",
    "    return out_scores\n",
    "\n",
    "# Now you are here: Just need to make sure that you don't count a point off if there were no relations of that type to be extracted from the document to begin with.\n",
    "\n",
    "# results = top_k()\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb73b6e1-bb7b-4e52-84bc-8981878733db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = top_k(num_blanks=0, dr=True, max_doc=1000, masks=True, n_passes=1, easymode=True)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0d540e-b491-4011-97fc-a1b493e8189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def domain_range(docred=None, rel_info=None, thresh=0.05):\n",
    "#     # dir = \"/data/git/text2kg2023-rilca-util\"#\\\\res\\\\eswc2023-results\"\n",
    "#     if not docred:\n",
    "#         docred = read_docred(dset='train')\n",
    "#     domain = defaultdict(Counter)\n",
    "#     range = defaultdict(Counter)\n",
    "#     for d, doc in enumerate(docred):\n",
    "#         for label in doc[\"labels\"]:\n",
    "#             r = label['r']\n",
    "#             domain[r].update(get_entity_type(doc, label['h']))\n",
    "#             range[r].update(get_entity_type(doc, label['t']))\n",
    "#     if rel_info:\n",
    "#         for p in domain:\n",
    "#             rel_info[p]['domain'] = threshold_counter(domain[p], thresh)\n",
    "#             rel_info[p]['range'] = threshold_counter(range[p], thresh)\n",
    "#         return rel_info\n",
    "#     else:\n",
    "#         out_domain = {}\n",
    "#         out_range = {}\n",
    "#         for p in domain:\n",
    "#             out_domain[p] = threshold_counter(domain[p], thresh)\n",
    "#             out_range[p] = threshold_counter(range[p], thresh)\n",
    "#         return out_domain, out_range\n",
    "import json\n",
    "\n",
    "rel_info = dict()\n",
    "\n",
    "for task in [\"docred\", \"biored\"]:\n",
    "    with open(f'/home/riley/understanding-pll/data/{task}/rel_info_full.json', 'r') as rel_info_file:\n",
    "        rel_info[task] = json.load(rel_info_file)\n",
    "\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c05d5ba-61bb-41e3-9db0-41d962a59ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Association'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_info['biored']['Association']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a310a5-b15b-4c77-a3a1-798cd34a54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "flipsort = False\n",
    "max_doc = 1000\n",
    "\n",
    "\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "def inner_work(folder, task_name, data_set, max_doc, num_blanks, masks, n_passes, dr, easymode, model_name):\n",
    "    res, lens = counts_to_percent(top_k(folder=folder, task_name=task_name, data_set=data_set, dr=dr, num_blanks=num_blanks, max_doc=max_doc, masks=masks, n_passes=n_passes, easymode=easymode, model_name=model_name))\n",
    "    output = []\n",
    "    if res['total_docs'] > 0:\n",
    "        output.append(f\"<h1>{task_name.upper()} {data_set.upper()} Test: {model_name} {'MASK' if masks else 'Blanked Entities'}, {num_blanks} blanks, {n_passes} passes<br/>First {min(res['total_docs'], max_doc)} docs.<br/>\")\n",
    "        output.append(f\"D&R restrictions are {'ON' if dr else 'OFF'}.</h1><br/>\")\n",
    "        rels = sorted((k for k in res.keys() if k != 'total_docs'), key=lambda x: int(x.split(' ')[0][1:])) if task_name == 'docred' else sorted(res.keys())\n",
    "        for rel in rels:\n",
    "            if rel == 'total_docs':\n",
    "                continue\n",
    "            rr = res[rel]\n",
    "            rr2 = {(outerKey, innerKey): values for outerKey, innerDict in rr.items() for innerKey, values in innerDict.items()}\n",
    "            df = pd.DataFrame(rr2).astype('float')\n",
    "            #   .format_index(str.upper, axis=1) \\\n",
    "            #   .relabel_index([\"row 1\", \"row 2\"], axis=0)\n",
    "            if rel in lens:\n",
    "                output.append(f'<h2>{rel} ({rel_info[task_name][rel][\"name\"]}) after {lens[rel]} docs</h2><br/>')\n",
    "                output.append(df.style.background_gradient(cmap=cm, vmin=0.0, vmax=100.0).format('{:.1f}').to_html())\n",
    "                output.append('<br/>')\n",
    "    return output\n",
    "\n",
    "# t = tqdm.tqdm(total=(2*2*2*2*1*1*3))\n",
    "res_map = {}\n",
    "\n",
    "with open('dev_answers_domain_range.pickle', 'rb') as domain_range_pickle:\n",
    "    dr_restricted_docred = pickle.load(domain_range_pickle)\n",
    "    \n",
    "with open('dev_answers_domain_range_biored.pickle', 'rb') as domain_range_pickle:\n",
    "    dr_restricted_biored = pickle.load(domain_range_pickle)\n",
    "\n",
    "while True:\n",
    "    # t.reset()\n",
    "    now = datetime.now()\n",
    "    for task_name, dr in [(\"docred\", dr_restricted_docred)]:#, (\"biored\", dr_restricted_biored)]:\n",
    "        for data_set in [\"train\", \"dev\"]:\n",
    "            for num_passes in range(0, 3):  # range(0, 5):\n",
    "                for num_blanks in range(0, 1):  # range(0, 5):\n",
    "                    for fs in [True]:\n",
    "                        for em in [True]:\n",
    "                            for model in [\"bert-base-cased\", \"bert-large-cased\", \"BiomedNLP-PubMedBERT-large-uncased-abstract\", \"biobert-large-cased-v1.1\", \"roberta-large\"]:\n",
    "                                op = inner_work('20240708', task_name=task_name, data_set=data_set, max_doc=max_doc, num_blanks=num_blanks, masks=True, n_passes=num_passes, dr=dr, easymode=em, model_name=model)\n",
    "                                if len(op) > 0:\n",
    "                                    res_map[(task_name, data_set, model, num_passes, num_blanks, fs)] = op\n",
    "                                    filename = \"index.html\" if task_name == \"docred\" else \"biored.html\"\n",
    "                                    with open(f'/home/riley/{filename}', 'w', encoding='utf8') as resfile:\n",
    "                                        resfile.write(f'<!DOCTYPE html><html><head><title>Experimental Results</title><meta http-equiv=\"refresh\" content=\"60\"/></head><body>')\n",
    "                                        resfile.write(f'<h0>Last updated: {now}</h0>')\n",
    "                                        for res in res_map:\n",
    "                                            if res[0] == task_name:\n",
    "                                                resfile.writelines(res_map[res])\n",
    "                                        resfile.write(\"</body></html>\")\n",
    "                                        resfile.flush()\n",
    "                                    ! scp /home/riley/$filename riley@reyncke.dev:~/public_html >> /dev/null\n",
    "                                # t.update()\n",
    "    time.sleep(60)\n",
    "# t.close()\n",
    "# df.style\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25e04c-b0bb-4c60-aeef-f507a57c2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next steps:\n",
    " >. Filter by domain/range, see how that changes things. Are the results visibly different? \n",
    "    - Doubles scores as expected, but does not do much else.\n",
    " >. Run a basic experiment using the original tokens. Does that look like the old results? VERIFY the new results.\n",
    " >. New nonlinearity: relu plus softmax\n",
    " >. New nonlinearity: top-k plus softmax (correctly)\n",
    " >. New scoring: Partial-PLL. PLL but only when the token ID is > -1 (Hard)\n",
    " <. Run the experiment using RoBERTa. Does that do anything?\n",
    " X. Run the experiment using BERT cased/uncased? Whatever isn't being used?\n",
    " 7. Combine all forward passes into the same experiment. Then it's cheap to change num_passes.\n",
    " \n",
    "\n",
    "Oh, that's a lot of next steps.\n",
    "\n",
    "\n",
    "Make the results page prettier? Filter options?\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f1cbc-d47d-49b1-932f-c11785bec4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Today (May 20):\n",
    "1. Make sure that the domain and range filters are applied to MENTION types, not entity types.\n",
    "2. Implement Partial PLL (first for -1 in place of all mentions)\n",
    "3. Implement Partial PLL (now for original inputs)\n",
    "4. Implement Dot Product (PLL but generalized)\n",
    "5. Combine all forward passes in a more unified way\n",
    "6. Apply filtering earlier.\n",
    "7. Try RoBERTa\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b6445-ae49-444e-bb8b-b1670e6ebd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = \"fdfd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c35a69-de0c-4fda-a0da-7e90ef013272",
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e11cfb-93e6-4229-8a45-7ba86ccbda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14041b-a42a-4a91-b825-b3901da18928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
